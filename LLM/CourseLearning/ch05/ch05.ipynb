{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 文本分类的微调"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ef8604639765aef"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.0\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.3.1\n",
      "tensorflow version: 2.16.1\n",
      "pandas version: 2.2.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\",\n",
    "        \"pandas\"\n",
    "        ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:18:51.852176400Z",
     "start_time": "2024-06-28T03:18:51.806297300Z"
    }
   },
   "id": "da0fc30e3953a3d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Alt text](../../../img/LLM/ch05/classify_finetune.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61709d1baf927cbe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1、不同类别的微调"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9500b15707949a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 微调语言模型最常见的方法是指令微调和分类微调\n",
    "- 下面描述的指令微调是下一章的主题\n",
    "\n",
    "![Alt text](../../../img/LLM/ch05/instruction_finetuning.png)\n",
    "\n",
    "- 如果你有机器学习的背景，本章的主题是分类微调，这是一个你可能已经熟悉的过程——例如，它类似于训练卷积网络来对手写数字进行分类\n",
    "- 在分类微调中，我们有特定数量的类标签（例如，“垃圾邮件”和“非垃圾邮件”），模型可以输出这些标签\n",
    "- 分类微调模型只能预测它在训练中看到的类（例如，“垃圾邮件”或“非垃圾邮件”），而指令微调模型通常可以执行许多任务\n",
    "- 我们可以把分类微调模型看作是一个非常专业的模型；在实践中，创建一个专门的模型要比创建一个在许多不同任务上表现良好的多面手模型容易得多\n",
    "\n",
    "![Alt text](../../../img/LLM/ch05/classify_finetuning.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0c6d2351374de56"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2、数据集准备"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "866c88d9954e57f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Alt text](../../../img/LLM/ch05/stage1_classify_finetune.png)\n",
    "\n",
    "- 本节准备用于分类微调的数据集\n",
    "- 我们使用由垃圾邮件和非垃圾邮件组成的数据集来微调LLM以对其进行分类\n",
    "- 首先，我们下载并解压缩数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70b68facaec71814"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection\\SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # Downloading the file\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # Unzipping the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Add .tsv file extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:18:58.218393700Z",
     "start_time": "2024-06-28T03:18:58.202405100Z"
    }
   },
   "id": "d2a0c616b7c0ba9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 数据集被保存为一个以制表符分隔的文本文件，我们可以将其加载到Panda DataFrame中"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac7b25d2c6c3e9c6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "     Label                                               Text\n0      ham  Go until jurong point, crazy.. Available only ...\n1      ham                      Ok lar... Joking wif u oni...\n2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3      ham  U dun say so early hor... U c already then say...\n4      ham  Nah I don't think he goes to usf, he lives aro...\n...    ...                                                ...\n5567  spam  This is the 2nd time we have tried 2 contact u...\n5568   ham               Will ü b going to esplanade fr home?\n5569   ham  Pity, * was in mood for that. So...any other s...\n5570   ham  The guy did some bitching but I acted like i'd...\n5571   ham                         Rofl. Its true to its name\n\n[5572 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5567</th>\n      <td>spam</td>\n      <td>This is the 2nd time we have tried 2 contact u...</td>\n    </tr>\n    <tr>\n      <th>5568</th>\n      <td>ham</td>\n      <td>Will ü b going to esplanade fr home?</td>\n    </tr>\n    <tr>\n      <th>5569</th>\n      <td>ham</td>\n      <td>Pity, * was in mood for that. So...any other s...</td>\n    </tr>\n    <tr>\n      <th>5570</th>\n      <td>ham</td>\n      <td>The guy did some bitching but I acted like i'd...</td>\n    </tr>\n    <tr>\n      <th>5571</th>\n      <td>ham</td>\n      <td>Rofl. Its true to its name</td>\n    </tr>\n  </tbody>\n</table>\n<p>5572 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:01.675872100Z",
     "start_time": "2024-06-28T03:18:59.985587300Z"
    }
   },
   "id": "9da08162c17ea90b"
  },
  {
   "cell_type": "markdown",
   "source": [
    " - 当我们检查类分布时，我们发现数据中包含“ham”（即“非垃圾邮件”）的频率远高于“垃圾邮件”"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1480437bc24ea53d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:03.123580800Z",
     "start_time": "2024-06-28T03:19:03.099645100Z"
    }
   },
   "id": "8dd9a080e7d5c67a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 为了简单起见，而且因为出于教育目的，我们更喜欢小的数据集（这将使更快地微调LLM成为可能），我们对数据集进行了子采样（欠采样），使其包含每个类的747个实例\n",
    "- （除了采样不足之外，还有其他几种处理类平衡的方法，但它们不在LLM书籍的范围内；您可以在不平衡学习用户指南中找到示例和更多信息）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7274b301203b1053"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:04.706161Z",
     "start_time": "2024-06-28T03:19:04.688181500Z"
    }
   },
   "id": "650959fdd824ddbf"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:05.895745300Z",
     "start_time": "2024-06-28T03:19:05.879760100Z"
    }
   },
   "id": "419a7d0ddf6d8bb6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 现在，让我们定义一个函数，将数据集随机划分为训练、验证和测试集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14cab16f6a7ee749"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    \n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "    \n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:06.909411100Z",
     "start_time": "2024-06-28T03:19:06.854558800Z"
    }
   },
   "id": "625bc145e33bbada"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3、创建数据加载程序"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52e2ab171947e895"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 请注意，文本消息具有不同的长度；如果我们想在一批中组合多个训练示例，我们必须\n",
    "    - 将所有消息截断为数据集中或批处理中最短消息的长度\n",
    "    - 将所有消息填充到数据集中或批处理中最长消息的长度\n",
    "- 我们选择选项2，并将所有消息填充到数据集中最长的消息中\n",
    "- 为此，我们使用<|endoftext|>作为填充标记，如第2章所述\n",
    "\n",
    "![Alt text](../../../img/LLM/ch05/token_padding.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b94754bae9acc952"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:11.514939500Z",
     "start_time": "2024-06-28T03:19:10.354243100Z"
    }
   },
   "id": "243d87b4f767b095"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 下面的SpamDataset类标识训练数据集中最长的序列，并将填充标记添加到其他序列以匹配该序列长度"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "598afb2efc9130f0"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "        \n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "            \n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "      \n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encode_length = len(encoded_text)\n",
    "            if encode_length > max_length:\n",
    "                max_length = encode_length\n",
    "        return max_length"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:19.357917100Z",
     "start_time": "2024-06-28T03:19:13.006645500Z"
    }
   },
   "id": "df52cf0364b2a7f2"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:20.665216700Z",
     "start_time": "2024-06-28T03:19:20.630310800Z"
    }
   },
   "id": "335ce59f83b25940"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 我们还将验证和测试集添加到最长的训练序列中\n",
    "- 请注意，比最长训练示例更长的验证和测试集样本将通过SpamDataset代码中的encode_text[：self.max_length]截断\n",
    "- 这种行为是完全可选的，如果我们在验证和测试集的情况下都将max_length设置为None，它也会很好地工作"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86fc47ba25ef8a0a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:22.631973600Z",
     "start_time": "2024-06-28T03:19:22.593081200Z"
    }
   },
   "id": "84bfe3d2148b3138"
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来，我们使用数据集实例化数据加载程序，这与前几章中创建数据加载程序类似\n",
    "\n",
    "![Alt text](../../../img/LLM/ch05/dataloader.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dcc0860faba87c4"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:24.587889500Z",
     "start_time": "2024-06-28T03:19:24.551986100Z"
    }
   },
   "id": "74ec7118f00aa807"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 作为验证步骤，我们对数据加载程序进行迭代，并确保批次中每个包含8个训练示例，其中每个训练示例由120个token组成"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "338c6e1dd7e9a84b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions:\", target_batch.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:26.140688500Z",
     "start_time": "2024-06-28T03:19:26.020978700Z"
    }
   },
   "id": "b06146e94ba37b10"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 最后，让我们打印每个数据集中的批次总数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23ba5e73d6bbffae"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:27.462675600Z",
     "start_time": "2024-06-28T03:19:27.442729500Z"
    }
   },
   "id": "5f4d1d85fce837a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4、使用预先训练的权重初始化模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad67a28196f851b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 在本节中，我们初始化上一章中使用的预训练模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de0f6df15323909b"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"embedding_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"embedding_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"embedding_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"embedding_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:32.099033100Z",
     "start_time": "2024-06-28T03:19:32.089031500Z"
    }
   },
   "id": "6d9853b583350a56"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": "GPTModel(\n  (token_emb): Embedding(50257, 768)\n  (pos_emb): Embedding(1024, 768)\n  (drop_emb): Dropout(p=0.0, inplace=False)\n  (transformer_blocks): Sequential(\n    (0): TransformerBlock(\n      (attention): MultiHeadAttention(\n        (W_q): Linear(in_features=768, out_features=768, bias=True)\n        (W_k): Linear(in_features=768, out_features=768, bias=True)\n        (W_v): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ffn): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (1): TransformerBlock(\n      (attention): MultiHeadAttention(\n        (W_q): Linear(in_features=768, out_features=768, bias=True)\n        (W_k): Linear(in_features=768, out_features=768, bias=True)\n        (W_v): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ffn): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (2): TransformerBlock(\n      (attention): MultiHeadAttention(\n        (W_q): Linear(in_features=768, out_features=768, bias=True)\n        (W_k): Linear(in_features=768, out_features=768, bias=True)\n        (W_v): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ffn): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (3): TransformerBlock(\n      (attention): MultiHeadAttention(\n        (W_q): Linear(in_features=768, out_features=768, bias=True)\n        (W_k): Linear(in_features=768, out_features=768, bias=True)\n        (W_v): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ffn): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (4): TransformerBlock(\n      (attention): MultiHeadAttention(\n        (W_q): Linear(in_features=768, out_features=768, bias=True)\n        (W_k): Linear(in_features=768, out_features=768, bias=True)\n        (W_v): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ffn): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (5): TransformerBlock(\n      (attention): MultiHeadAttention(\n        (W_q): Linear(in_features=768, out_features=768, bias=True)\n        (W_k): Linear(in_features=768, out_features=768, bias=True)\n        (W_v): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ffn): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (6): TransformerBlock(\n      (attention): MultiHeadAttention(\n        (W_q): Linear(in_features=768, out_features=768, bias=True)\n        (W_k): Linear(in_features=768, out_features=768, bias=True)\n        (W_v): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ffn): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (7): TransformerBlock(\n      (attention): MultiHeadAttention(\n        (W_q): Linear(in_features=768, out_features=768, bias=True)\n        (W_k): Linear(in_features=768, out_features=768, bias=True)\n        (W_v): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ffn): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (8): TransformerBlock(\n      (attention): MultiHeadAttention(\n        (W_q): Linear(in_features=768, out_features=768, bias=True)\n        (W_k): Linear(in_features=768, out_features=768, bias=True)\n        (W_v): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ffn): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (9): TransformerBlock(\n      (attention): MultiHeadAttention(\n        (W_q): Linear(in_features=768, out_features=768, bias=True)\n        (W_k): Linear(in_features=768, out_features=768, bias=True)\n        (W_v): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ffn): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (10): TransformerBlock(\n      (attention): MultiHeadAttention(\n        (W_q): Linear(in_features=768, out_features=768, bias=True)\n        (W_k): Linear(in_features=768, out_features=768, bias=True)\n        (W_v): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ffn): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n    (11): TransformerBlock(\n      (attention): MultiHeadAttention(\n        (W_q): Linear(in_features=768, out_features=768, bias=True)\n        (W_k): Linear(in_features=768, out_features=768, bias=True)\n        (W_v): Linear(in_features=768, out_features=768, bias=True)\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ffn): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_shortcut): Dropout(p=0.0, inplace=False)\n    )\n  )\n  (final_norm): LayerNorm()\n  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from ch05 import GPTModel, load_weights_into_gpt\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size, \"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:19:58.107691300Z",
     "start_time": "2024-06-28T03:19:32.951846Z"
    }
   },
   "id": "6b4c35dba6ffa2af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 为了确保模型被正确加载，让我们仔细检查一下它是否生成了连贯的文本"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "293e83aeeafe7798"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work.\n",
      "\n",
      "The second step is to understand the importance of your work.\n",
      "\n",
      "The third step is to understand the importance of your work.\n",
      "\n",
      "The fourth step is\n"
     ]
    }
   ],
   "source": [
    "from ch05 import generate_text_simple, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "text1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text1, tokenizer),\n",
    "    max_new_tokens=50,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:22:32.726034600Z",
     "start_time": "2024-06-28T03:22:27.034596200Z"
    }
   },
   "id": "9b5768d53cd379f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 在我们将模型作为分类器进行微调之前，让我们看看该模型是否已经可以通过提示对垃圾邮件进行分类"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e73f6d712f31b98"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "text2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG['context_length']\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T06:41:38.152056800Z",
     "start_time": "2024-06-28T06:41:30.889987700Z"
    }
   },
   "id": "3f5d948c857ad538"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 正如我们所看到的，该模型在以下说明方面不是很好\n",
    "- 这是意料之中的事，因为它只是经过了预训练，而不是指令微调（指令微调将在下一章中介绍）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fa76ce03ad74a8d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5、添加分类head"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b10ead215121d53d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Alt text](../../../img/LLM/ch05/classify_head.png)\n",
    "\n",
    "- 在本节中，我们将修改预训练的LLM，使其为分类微调做好准备\n",
    "- 让我们先来看看模型体系结构"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52105f069ab48a4d"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (token_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T06:44:23.087300500Z",
     "start_time": "2024-06-28T06:44:22.986544400Z"
    }
   },
   "id": "bb45f975991aa528"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 在上面，我们可以看到我们在第3章中实现的架构\n",
    "- 目标是替换和微调输出层\n",
    "- 为了实现这一点，我们首先冻结模型，这意味着我们使所有层都不可训练"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41508d9d28dc6b09"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T06:45:33.850792800Z",
     "start_time": "2024-06-28T06:45:33.684633200Z"
    }
   },
   "id": "4dae683dda228642"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 然后，我们替换输出层（model.out_head），它最初将层输入映射到50257个维度（词汇表的大小）\n",
    "- 由于我们对二进制分类的模型进行了微调（预测了两个类，“垃圾邮件”和“非垃圾邮件”），我们可以替换输出层，如下所示，默认情况下可以进行训练\n",
    "- 请注意，我们使用BASE_CONFIG[“emb_dim”]（在“gpt2 small（124M）”模型中等于768）来保持下面的代码更通用"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fd754880e999274"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"embedding_dim\"], out_features=num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T06:47:57.541059Z",
     "start_time": "2024-06-28T06:47:57.412405400Z"
    }
   },
   "id": "7b189aa510d3ee66"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 从技术上讲，只训练输出层就足够了\n",
    "- 然而，正如我在微调大型语言模型中发现的那样，实验表明微调附加层可以显著提高性能\n",
    "- 因此，我们还使最后一个Transformer块和将最后一个Transformer块连接到输出层的最后一个LayerNorm模块可训练\n",
    "\n",
    "![Alt text](../../../img/LLM/ch05/finetune_architecture.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87cc4ebfe983ae2"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "for param in model.transformer_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T06:50:39.652891900Z",
     "start_time": "2024-06-28T06:50:39.521214400Z"
    }
   },
   "id": "bd7ed60590afbda3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 我们仍然可以使用与前几章中类似的模型\n",
    "- 例如，让我们给它一些文本输入"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14078e2408602b02"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T06:51:52.707014400Z",
     "start_time": "2024-06-28T06:51:52.580351500Z"
    }
   },
   "id": "1f6ce3dc56d993ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 与前几章不同的是，它现在有两个输出维度，而不是50257"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8be44dc67950d2b8"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T06:54:18.462334200Z",
     "start_time": "2024-06-28T06:54:16.678741Z"
    }
   },
   "id": "5aab011d48c5665b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "99d3c0b05346cc08"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "llm",
   "language": "python",
   "display_name": "LLM"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
