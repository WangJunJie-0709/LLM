{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 理解Embedding层和线性层的区别"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ffd73194f7adefd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch中的嵌入层实现了与执行矩阵乘法的线性层相同的功能;\n",
    "我们使用嵌入层的原因是计算效率,我们将使用PyTorch中的代码示例逐步研究这种关系"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cd6cae8cede85d3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T09:38:49.531015700Z",
     "start_time": "2024-06-13T09:38:49.518789500Z"
    }
   },
   "id": "194bf58d230de8a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embedding层"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d2bffb0a716fe83"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "idx = torch.tensor([2, 3, 1])\n",
    "\n",
    "num_idx = max(idx) + 1\n",
    "\n",
    "output_dim = 5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T09:38:23.386901600Z",
     "start_time": "2024-06-13T09:38:23.312102800Z"
    }
   },
   "id": "578f70a75362d06c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "embedding_layers = nn.Embedding(num_idx, output_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T09:39:05.079516600Z",
     "start_time": "2024-06-13T09:39:05.057573700Z"
    }
   },
   "id": "8a9d3332a955845e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  1.5810],\n        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015],\n        [ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953]], requires_grad=True)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layers.weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T09:39:12.442835200Z",
     "start_time": "2024-06-13T09:39:12.374020400Z"
    }
   },
   "id": "c70a332d05565608"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n       grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layers(torch.tensor([1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T09:39:34.449004300Z",
     "start_time": "2024-06-13T09:39:34.355229100Z"
    }
   },
   "id": "a39c170f4665775b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以可视化一下看看发生了什么\n",
    "\n",
    "![Alt text](../../../img/LLM/ch01/embedding%5B1%5D.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7b9358d503d64e2"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315]],\n       grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layers(torch.tensor([2]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T02:50:18.219665300Z",
     "start_time": "2024-06-14T02:50:18.134894600Z"
    }
   },
   "id": "1f55849c06d17014"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Alt text](../../../img/LLM/ch01/embedding%5B2%5D.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68706a7ac266b10a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在，让我们转换之前定义的所有训练示例："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12e3038f3d48e0ea"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n       grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([2, 3, 1])\n",
    "embedding_layers(idx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T02:51:13.471671700Z",
     "start_time": "2024-06-14T02:51:13.432775700Z"
    }
   },
   "id": "948fa1c5b1abc4bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Alt text](../../../img/LLM/ch01/embedding_lookup.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f8141256d27a1b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deb9bfe478e516aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 现在，我们将证明上面的嵌入层实现了与nn完全相同的功能。PyTorch中一个热编码表示上的线性层\n",
    "- 首先，让我们将token ID转换为一个热表示："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b17ccadab8d14d3"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 0, 1, 0],\n        [0, 0, 0, 1],\n        [0, 1, 0, 0]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = torch.nn.functional.one_hot(idx)\n",
    "onehot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T02:53:40.381142100Z",
     "start_time": "2024-06-14T02:53:40.275426900Z"
    }
   },
   "id": "2a72e83586dd0f9c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来，我们初始化一个线性层，它进行矩阵乘法 XWT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9c0aca60dd02303"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-0.2039,  0.0166, -0.2483,  0.1886],\n        [-0.4260,  0.3665, -0.3634, -0.3975],\n        [-0.3159,  0.2264, -0.1847,  0.1871],\n        [-0.4244, -0.3034, -0.1836, -0.0983],\n        [-0.3814,  0.3274, -0.1179,  0.1605]], requires_grad=True)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "linear = torch.nn.Linear(num_idx, output_dim, bias=False)\n",
    "linear.weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T02:54:52.152801Z",
     "start_time": "2024-06-14T02:54:51.983254200Z"
    }
   },
   "id": "c61c10a3a8737512"
  },
  {
   "cell_type": "markdown",
   "source": [
    "请注意，PyTorch中的线性层也使用小的随机权重进行初始化；要直接将其与上面的嵌入层进行比较，我们必须使用相同的小随机权重，这就是为什么我们在这里重新分配它们："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66a8f4db94845245"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "linear.weight = torch.nn.Parameter(embedding_layers.weight.T.detach())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T02:55:36.488373Z",
     "start_time": "2024-06-14T02:55:36.458455Z"
    }
   },
   "id": "8b0f3cbc171857cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在，我们可以在输入的一个热编码表示上使用线性层："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f37e23af3877228"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]], grad_fn=<MmBackward0>)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(onehot.float())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T02:56:03.165986700Z",
     "start_time": "2024-06-14T02:56:03.020375800Z"
    }
   },
   "id": "ef30f4d6200507e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "正如我们所看到的，这与我们使用嵌入层时得到的结果完全相同："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4697be853405796d"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n       grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layers(idx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T02:56:15.072356400Z",
     "start_time": "2024-06-14T02:56:15.017503800Z"
    }
   },
   "id": "a7775ec62799868e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "第一个训练示例的token ID的以下计算：\n",
    "\n",
    "![Alt text](../../../img/LLM/ch01/example_first.png)\n",
    "\n",
    "![Alt text](../../../img/LLM/ch01/example_second.png)\n",
    "\n",
    "由于每一个热编码行中除一个索引外的所有索引都为0（按设计），因此此矩阵乘法基本上与查找一个热元素相同\n",
    "在一个热编码上使用矩阵乘法相当于嵌入层查找，但如果我们使用大的嵌入矩阵，则可能效率低下，因为存在大量浪费的乘零运算"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ac8b840052591c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bc5a48adb9e0813f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "llm",
   "language": "python",
   "display_name": "LLM"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
