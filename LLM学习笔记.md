# 一、什么是大语言模型（LLM）

## 模型定义

大语言模型，如ChatGPT，正在今天取得技术世界的欢迎。根据Wikipedia的定义，LLM是这样解释的

- 大语言模型是一种因其能够实现通用语言理解和生成而显著的语言模型。LLM通过在计算密集型的自监督和半监督训练过程中学习文本文档的统计关系来获得这些能力。LLM是遵循变换架构的人工神经网络

LLM通过像书籍、网站或用户生成的内容这样的大型文本数据集进行训练。它们可以以自然的方式生成续写初始提示的新文本。

## 模型架构

在LLM出现之前，机器对神经网络的训练的确局限于使用相对较小的数据集，并且对上下文理解的能力非常有限。这意味着早期的模型无法像人类那样理解文本。

下面是一个图标，展示了模型训练过程中发生的事情

![image-20240428181331661](F:\LLM\photo\transformer架构.jpg)



# 二、LLM如何工作

文档补充：提示词->模型->响应

生成式：提示词->预训练微调模型->响应





## 预训练



## 微调



## 基于反馈的强化学习（RLHF）



## 提示词工程



# 三、Transformer架构

![image-20240428182221742](F:\LLM\photo\transformer底层架构.jpg)



## Encoder

![image-20240428203602098](F:\LLM\photo\Encoder.jpg)





## Decoder



